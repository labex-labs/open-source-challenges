# Introduction

The Local Outlier Factor (LOF) algorithm is an unsupervised machine learning method that is used to detect anomalies in data. It computes the local density deviation of a given data point with respect to its neighbors and considers as outliers the samples that have a substantially lower density than their neighbors.

In this lab, we will use LOF to detect outliers in a dataset.

> You can write code in `lab.ipynb`.

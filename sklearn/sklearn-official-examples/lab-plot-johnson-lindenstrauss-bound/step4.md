# Summary

In this lab, we explored the theoretical bounds of the Johnson-Lindenstrauss lemma for embedding with random projections and validated it empirically using Python scikit-learn. We plotted the minimum number of dimensions required to guarantee an `eps`-embedding for an increasing number of samples `n_samples`. We also validated the Johnson-Lindenstrauss bounds empirically on the 20 newsgroups text document dataset or on the digits dataset. We projected 300 documents with 100k features in total using a sparse random matrix to smaller Euclidean spaces with various values for the target number of dimensions `n_components`. We can see that for low values of `n_components` the distribution is wide with many distorted pairs and a skewed distribution while for larger values of `n_components` the distortion is controlled and the distances are well preserved by the random projection.

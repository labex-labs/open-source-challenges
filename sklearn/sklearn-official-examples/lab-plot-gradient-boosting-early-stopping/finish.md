# Summary

In this lab, we learned about early stopping in gradient boosting, which enables us to find the least number of iterations that are sufficient to build a model that generalizes well to unseen data. We compared the performance of a gradient boosting model with and without early stopping, and observed that early stopping can significantly reduce training time, memory usage, and prediction latency.

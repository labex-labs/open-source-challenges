# Decomposing Signals In Components

## Introduction

In this lab, we will explore the topic of decomposing signals into components using matrix factorization techniques provided by scikit-learn. We will cover techniques such as Principal Component Analysis (PCA), Independent Component Analysis (ICA), Non-negative Matrix Factorization (NMF), and more. This lab will guide you through the process of decomposing signals into their components step by step.

## Steps

### Step 1: Principal Component Analysis (PCA)

#### Exact PCA and probabilistic interpretation

Principal Component Analysis (PCA) is used to decompose a multivariate dataset into a set of successive orthogonal components that explain a maximum amount of variance. PCA can be implemented using the `PCA` class from scikit-learn. The `fit` method is used to learn the components, and the `transform` method can be used to project new data onto these components.

```python
from sklearn.decomposition import PCA

# Create a PCA object with n_components as the number of desired components
pca = PCA(n_components=2)

# Fit the PCA model to the data
pca.fit(data)

# Transform the data by projecting it onto the learned components
transformed_data = pca.transform(data)
```

### Step 2: Independent Component Analysis (ICA)

#### ICA for blind source separation

Independent Component Analysis (ICA) is used to separate mixed signals into their original source components. It assumes that the components are statistically independent and can be extracted through a linear unmixing process. ICA can be implemented using the `FastICA` class from scikit-learn.

```python
from sklearn.decomposition import FastICA

# Create an ICA object with n_components as the number of desired components
ica = FastICA(n_components=2)

# Fit the ICA model to the mixed signals
ica.fit(mixed_signals)

# Separate the mixed signals into the original source components
source_components = ica.transform(mixed_signals)
```

### Step 3: Non-negative Matrix Factorization (NMF)

#### NMF with the Frobenius norm

Non-negative Matrix Factorization (NMF) is an alternative approach to decomposition that assumes non-negative data and components. It finds a decomposition of the data into two matrices of non-negative elements by optimizing the distance between the data and the matrix product of the two matrices. NMF can be implemented using the `NMF` class from scikit-learn.

```python
from sklearn.decomposition import NMF

# Create an NMF object with n_components as the number of desired components
nmf = NMF(n_components=2)

# Fit the NMF model to the data
nmf.fit(data)

# Decompose the data into the two non-negative matrices
matrix_W = nmf.transform(data)
matrix_H = nmf.components_
```

### Step 4: Latent Dirichlet Allocation (LDA)

#### LDA for topic modeling

Latent Dirichlet Allocation (LDA) is a generative probabilistic model used for discovering abstract topics from a collection of documents. LDA assumes that documents are a mixture of topics and that words are generated by these topics. LDA can be implemented using the `LatentDirichletAllocation` class from scikit-learn.

```python
from sklearn.decomposition import LatentDirichletAllocation

# Create an LDA object with n_components as the number of desired topics
lda = LatentDirichletAllocation(n_components=5)

# Fit the LDA model to the document-term matrix
lda.fit(document_term_matrix)

# Get the topic-term matrix and the document-topic matrix
topic_term_matrix = lda.components_
document_topic_matrix = lda.transform(document_term_matrix)
```

## Summary

In this lab, we explored various techniques for decomposing signals into their components. We learned about Principal Component Analysis (PCA), Independent Component Analysis (ICA), Non-negative Matrix Factorization (NMF), and Latent Dirichlet Allocation (LDA). These techniques are widely used in various applications such as dimensionality reduction, blind source separation, topic modeling, and more. By applying these techniques, we can gain insights and extract meaningful information from high-dimensional signals and datasets.
